<!DOCTYPE html>
<!-- Authors: Roxana Daneshjou -->
<html lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16">
  <meta charset="utf-8">
  <title>Red Teaming Large Language Models in Medicine</title>
  <meta name="description" content="Red Teaming Large Language Models in Medicine: Real-World Insights on Model Behavior"><!--#include file="lagunita.html" -->
  <!--Lagunita Theme (TODO: Server side include)-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="lagunita/css/bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="lagunita/css/base.min.css?v=0.1" type="text/css">
  <link rel="stylesheet" href="lagunita/css/custom.css?v=0.1" type="text/css">
  <!--End Lagunita Theme-->
  <link rel="icon" type="image/ico" href="favicon.ico">
</head>
<body class="site-slogan">
  <!--#include file="header.html" -->
  <!--Header (TODO: Server side include)-->
  <div id="top">
    <div class="container">
      <!--=== Skip links ===-->
      <div id="skip">
        <a href="#content" onclick="$('#content').focus()">Skip to content</a>
      </div><!-- /Skip links -->
    </div>
  </div>
  <div id="brandbar">
    <div class="container">
      <a href="http://www.stanford.edu"><img src="lagunita/images/brandbar-stanford-logo%402x.png" alt="Stanford University" width="152" height="23"></a>
    </div><!-- .container end -->
  </div>
  <div id="header" class="clearfix" role="banner">
    <div class="container">
      <div class="row">
        <div class="col-md-8">
          <div id="signature">
            <div id="site-name">
              <a href="https://daneshjoulab.github.io/Red-Teaming-Dataset/"><span id="site-name-1">Red Teaming Large Language Models in Medicine</span></a>
            </div>
            <div id="site-slogan">
              <a href="https://daneshjoulab.github.io/Red-Teaming-Dataset/"><span id="site_slogan">Real-World Insights on Model Behavior</span></a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div><!-- Menu -->
  <div id="mainmenu" class="clearfix" role="navigation">
    <div class="container">
      <div class="navbar navbar-default">
        <!-- main navigation -->
         <button type="button" class="navbar-toggle btn-navbar" data-toggle="collapse" data-target=".navbar-collapse"> <span class="menu-text">Menu</span></button> <!-- /nav-collapse -->
        <div class="navbar-collapse collapse">
          <div role="navigation">
            <div id="primary-nav">
              <ul class="nav navbar-nav" aria-label="primary navigation">
                <li id="nav-1">
                  <a href="index.html">Home</a>
                </li>
                <li id="nav-2">
                  <a href="index.html#intro">Introduction</a>
                </li>
                <li id="nav-4">
                  <a href="index.html#dataset">Dataset</a>
                </li>
                <li id="nav-7">
                  <a href="index.html#paper">Paper</a>
                </li>
              </ul>
            </div>
          </div>
        </div><!-- /nav-collapse -->
      </div><!-- /navbar -->
    </div><!-- /container -->
  </div><!-- /mainmenu -->
  <!--End Header-->
  <div id="intro" class="container" role="introduction" tabindex="0">
    <h2>Introduction</h2>
    <p>The integration of large language models (LLMs) in healthcare offers immense opportunity to streamline healthcare tasks, but also carries risks such as response accuracy and the perpetuation of biases. To address this, we conducted a red-teaming exercise to assess LLMs in healthcare and developed a dataset of clinically relevant scenarios for future teams to use. Red teaming, the practice of adversarially exposing unexpected or undesired model behaviors, is critical towards improving equity and accuracy of large language models, but non-model creator-affiliated red teaming is scant in healthcare. We share insights for constructing red teaming prompts, and present our benchmark for iterative model assessments.</p><br>
    <p><h2 id="dataset">Dataset</h2>
    <p><b>Labeling:</b> We convened 80 multi-disciplinary experts to evaluate the performance of popular LLMs across multiple medical scenarios. Teams composed of clinicians, medical, and engineering students, and technical professionals stress-tested LLMs with real world clinical use cases. Teams were given a framework comprising four categories to analyze for inappropriate responses: Safety, Privacy, Hallucinations, and Bias. Six medically trained reviewers subsequently reanalyzed the prompt-response pairs, with dual reviewers for each prompt and a third to resolve discrepancies.</p><br>
    <p><b>Dataset:</b> Of 376 unique prompts (1504 responses), 20.1% were inappropriate (GPT-3.5: 25.8%; GPT-4.0: 16%; GPT-4.0 with Internet: 17.8%). Subsequently, we show the utility of our benchmark by testing GPT-4o, a model released after our event (20.4% inappropriate). 21.5% of responses appropriate with GPT-3.5 were inappropriate in updated models. </p><br>
    <a href="https://raw.githubusercontent.com/DaneshjouLab/Red-Teaming-Dataset/main/SupplA_RedTeamining Datasheet.pdf" download>Download the datasheet for the dataset by clicking here</a><br>
    <a href="https://raw.githubusercontent.com/DaneshjouLab/Red-Teaming-Dataset/main/RedTeamining_final_dataset.csv" download>Download the dataset by clicking here</a><br>
    <a href="https://raw.githubusercontent.com/DaneshjouLab/Red-Teaming-Dataset/main/Supp. File B_ Red Teaming Synthetic Notes.pdf" download>Download the synthetic notes provided to participants by clicking here</a><br>
    <br>
    <h2 id="paper">Paper</h2>
    <p><a href="https://www.medrxiv.org/content/10.1101/2024.04.05.24305411v1">***preprint available here***</a><br>Crystal T. Chang<sup>*</sup>, Hodan Farah<sup>*</sup>, Haiwen Gui<sup>*</sup>, Shawheen Justin Rezaei, Charbel Bou-Khalil, Ye-Jean Park, Akshay Swaminathan, Jesutofunmi A. Omiye, Akaash Kolluri, Akash Chaurasia, Alejandro Lozano, Alice Heiman, Allison Sihan Jia, Amit Kaushal, Angela Jia, Angelica Iacovelli, Archer Yang, Arghavan Salles, Arpita Singhal, Balasubramanian Narasimhan, Benjamin Belai, Benjamin H. Jacobson, Binglan Li, Celeste H. Poe, Chandan Sanghera, Chenming Zheng, Conor Messer, Damien Varid Kettud, Deven Pandya, Dhamanpreet Kaur, Diana Hla, Diba Dindoust, Dominik Moehrle, Duncan Ross, Ellaine Chou, Eric Lin, Fateme Nateghi Haredasht, Ge Cheng, Irena Gao, Jacob Chang, Jake Silberg, Jason A. Fries, Jenelle Jindal, Jiapeng Xu, Joe Jamison, John S. Tamaresis, Jonathan H Chen, Joshua Lazaro, Juan M. Banda, Julie J. Lee, Karen Ebert Matthy, Kirsten R. Steffner, Lu Tian, Luca Pegolotti, Malathi Srinivasan, Maniragav Manimaran, Matthew Schwede, Minghe Zhang, Minh Nguyen, Mohsen Fathzadeh, Qian Zhao, Rika Bajra, Rohit Khurana, Ruhana Azam, Rush Bartlett, Sang T. Truong, Scott L Fleming, Shriti Raj, Solveig Behr, Sonia Onyeka, Sri Muppidi, Tarek Bandali, Tiffany Y. Eulalio, Wenyuan Chen, Xuanyu Zhou, Yanan Ding, Ying Cui, Yuqi Tan, Yutong Liu, Nigam Shah, <b>Roxana Daneshjou</b>. arXiv(2024) </p>
    <p><sup>*</sup><i>These authors contributed equally as a co-first author to this manuscript, and are presented in alphabetical order</i></p>
    <p>For inquiries, contact us at <a href="mailto:%20roxanad@stanford.edu">roxanad@stanford.edu</a>.</p>

  </div>
  <!--Lagunita Theme (TODO: Server side include)-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <script src="lagunita/js/modernizr.custom.17475.js"></script>
  <script src="lagunita/js/bootstrap.min.js"></script>
  <script src="lagunita/js/base.js?v=1.0"></script>
  <script src="lagunita/js/custom.js"></script>
  <!--End Lagunita Theme-->
</body>
</html>

